"""Decoder-only transformer architecture.

This module contains the core LLM components:
- Multi-head self-attention
- Transformer blocks
- Decoder model
- Full LLM with embeddings and language modeling head
"""
